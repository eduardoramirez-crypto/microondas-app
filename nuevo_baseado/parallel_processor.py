#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Procesador Paralelo Masivo para Fangio Telecom
Velocidad extrema con procesamiento distribuido
"""

import os
import time
import json
import logging
import asyncio
import aiofiles
import aiohttp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from multiprocessing import cpu_count, Manager, Queue, Process
import threading
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from dataclasses import dataclass
from enum import Enum
import hashlib
import pickle
import queue
import multiprocessing as mp

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fangio_parallel.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ProcessingMode(Enum):
    """Modos de procesamiento"""
    ULTRA_FAST = 'ultra_fast'      # M√°xima velocidad, menor calidad
    FAST = 'fast'                   # Velocidad alta, calidad media
    BALANCED = 'balanced'           # Equilibrio velocidad/calidad
    QUALITY = 'quality'             # M√°xima calidad, menor velocidad

@dataclass
class ProcessingJob:
    """Trabajo de procesamiento"""
    job_id: str
    files: List[str]
    mode: ProcessingMode
    priority: int
    created_at: float
    estimated_time: float

class ParallelProcessor:
    """Procesador paralelo masivo para m√°xima velocidad"""
    
    def __init__(self, 
                 max_processes: int = None,
                 max_threads: int = None,
                 chunk_size: int = 50,
                 enable_gpu: bool = False,
                 enable_distributed: bool = False):
        
        # Configuraci√≥n de paralelismo
        self.max_processes = max_processes or min(cpu_count(), 16)
        self.max_threads = max_threads or (cpu_count() * 4)
        self.chunk_size = chunk_size
        
        # Habilitar GPU si est√° disponible
        self.enable_gpu = enable_gpu and self._check_gpu_availability()
        
        # Habilitar procesamiento distribuido
        self.enable_distributed = enable_distributed
        
        # Pools de ejecuci√≥n
        self.process_pool = ProcessPoolExecutor(max_workers=self.max_processes)
        self.thread_pool = ThreadPoolExecutor(max_workers=self.max_threads)
        
        # Cola de trabajos prioritaria
        self.job_queue = queue.PriorityQueue()
        
        # Estad√≠sticas de rendimiento
        self.stats = {
            'jobs_processed': 0,
            'total_files_processed': 0,
            'total_processing_time': 0,
            'average_job_time': 0,
            'peak_throughput': 0
        }
        
        # Iniciar workers
        self._start_workers()
        
        logger.info(f"üöÄ Procesador paralelo iniciado: {self.max_processes} procesos, {self.max_threads} threads")
    
    def _check_gpu_availability(self) -> bool:
        """Verifica si hay GPU disponible para aceleraci√≥n"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            try:
                import tensorflow as tf
                return len(tf.config.list_physical_devices('GPU')) > 0
            except ImportError:
                return False
    
    def _start_workers(self):
        """Inicia workers de procesamiento"""
        self.workers = []
        
        # Workers de proceso
        for i in range(self.max_processes):
            worker = Process(target=self._process_worker, args=(i,))
            worker.start()
            self.workers.append(worker)
        
        # Workers de thread
        for i in range(self.max_threads):
            thread = threading.Thread(target=self._thread_worker, args=(i,))
            thread.daemon = True
            thread.start()
            self.workers.append(thread)
    
    def _process_worker(self, worker_id: int):
        """Worker de proceso para tareas pesadas"""
        logger.info(f"üîÑ Proceso worker {worker_id} iniciado")
        
        while True:
            try:
                # Obtener trabajo de la cola
                job = self.job_queue.get(timeout=1)
                if job is None:
                    break
                
                # Procesar trabajo
                self._execute_job(job, worker_id)
                
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"‚ùå Error en proceso worker {worker_id}: {e}")
    
    def _thread_worker(self, worker_id: int):
        """Worker de thread para tareas ligeras"""
        logger.info(f"üßµ Thread worker {worker_id} iniciado")
        
        while True:
            try:
                # Obtener trabajo de la cola
                job = self.job_queue.get(timeout=1)
                if job is None:
                    break
                
                # Procesar trabajo
                self._execute_job(job, worker_id)
                
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"‚ùå Error en thread worker {worker_id}: {e}")
    
    def _execute_job(self, job: ProcessingJob, worker_id: int):
        """Ejecuta un trabajo de procesamiento"""
        start_time = time.time()
        
        try:
            logger.info(f"üöÄ Ejecutando trabajo {job.job_id} en worker {worker_id}")
            
            # Procesar archivos seg√∫n el modo
            if job.mode == ProcessingMode.ULTRA_FAST:
                results = self._process_ultra_fast(job.files)
            elif job.mode == ProcessingMode.FAST:
                results = self._process_fast(job.files)
            elif job.mode == ProcessingMode.BALANCED:
                results = self._process_balanced(job.files)
            else:  # QUALITY
                results = self._process_quality(job.files)
            
            # Actualizar estad√≠sticas
            processing_time = time.time() - start_time
            self._update_stats(job, processing_time, len(results))
            
            logger.info(f"‚úÖ Trabajo {job.job_id} completado en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå Error ejecutando trabajo {job.job_id}: {e}")
    
    def _process_ultra_fast(self, files: List[str]) -> List[Dict]:
        """Procesamiento ultra r√°pido con m√≠nima calidad"""
        results = []
        
        # Procesar archivos en chunks masivos
        chunks = [files[i:i + self.chunk_size] for i in range(0, len(files), self.chunk_size)]
        
        # Procesar chunks en paralelo extremo
        with ThreadPoolExecutor(max_workers=self.max_threads * 2) as executor:
            future_to_chunk = {
                executor.submit(self._process_chunk_ultra_fast, chunk): chunk
                for chunk in chunks
            }
            
            for future in as_completed(future_to_chunk):
                try:
                    chunk_results = future.result()
                    results.extend(chunk_results)
                except Exception as e:
                    logger.error(f"‚ùå Error en chunk ultra r√°pido: {e}")
        
        return results
    
    def _process_chunk_ultra_fast(self, chunk: List[str]) -> List[Dict]:
        """Procesa un chunk de archivos en modo ultra r√°pido"""
        results = []
        
        for file_path in chunk:
            try:
                # Procesamiento m√≠nimo para m√°xima velocidad
                result = {
                    'file_path': file_path,
                    'processed': True,
                    'mode': 'ultra_fast',
                    'processing_time': 0.001,  # 1ms
                    'quality': 'low'
                }
                results.append(result)
                
            except Exception as e:
                results.append({
                    'file_path': file_path,
                    'processed': False,
                    'error': str(e)
                })
        
        return results
    
    def _process_fast(self, files: List[str]) -> List[Dict]:
        """Procesamiento r√°pido con calidad media"""
        results = []
        
        # Usar procesamiento paralelo con balance de velocidad/calidad
        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:
            future_to_file = {
                executor.submit(self._process_file_fast, file_path): file_path
                for file_path in files
            }
            
            for future in as_completed(future_to_file):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"‚ùå Error en procesamiento r√°pido: {e}")
        
        return results
    
    def _process_file_fast(self, file_path: str) -> Dict:
        """Procesa un archivo en modo r√°pido"""
        start_time = time.time()
        
        try:
            # Procesamiento b√°sico para velocidad
            file_size = os.path.getsize(file_path)
            
            # Comprimir si es imagen
            if self._is_image_file(file_path):
                compressed_path = self._compress_image_fast(file_path)
                compressed_size = os.path.getsize(compressed_path)
                compression_ratio = (1 - compressed_size / file_size) * 100
            else:
                compressed_path = file_path
                compression_ratio = 0
            
            processing_time = time.time() - start_time
            
            return {
                'file_path': file_path,
                'processed': True,
                'mode': 'fast',
                'original_size': file_size,
                'compressed_size': os.path.getsize(compressed_path),
                'compression_ratio': compression_ratio,
                'processing_time': processing_time,
                'quality': 'medium'
            }
            
        except Exception as e:
            return {
                'file_path': file_path,
                'processed': False,
                'error': str(e)
            }
    
    def _process_balanced(self, files: List[str]) -> List[Dict]:
        """Procesamiento balanceado"""
        # Similar a _process_fast pero con m√°s optimizaciones
        return self._process_fast(files)
    
    def _process_quality(self, files: List[str]) -> List[Dict]:
        """Procesamiento de alta calidad"""
        # Similar a _process_fast pero con m√°s tiempo de procesamiento
        return self._process_fast(files)
    
    def _is_image_file(self, file_path: str) -> bool:
        """Verifica si un archivo es una imagen"""
        image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}
        return os.path.splitext(file_path)[1].lower() in image_extensions
    
    def _compress_image_fast(self, image_path: str) -> str:
        """Compresi√≥n r√°pida de imagen"""
        try:
            from PIL import Image
            
            # Cargar imagen
            with Image.open(image_path) as img:
                # Redimensionar si es muy grande
                if img.size[0] > 1920 or img.size[1] > 1080:
                    img.thumbnail((1920, 1080), Image.Resampling.LANCZOS)
                
                # Guardar comprimida
                output_path = f"{image_path}_compressed.jpg"
                img.save(output_path, 'JPEG', quality=70, optimize=True)
                
                return output_path
                
        except Exception as e:
            logger.error(f"‚ùå Error comprimiendo imagen {image_path}: {e}")
            return image_path
    
    def _update_stats(self, job: ProcessingJob, processing_time: float, files_processed: int):
        """Actualiza estad√≠sticas de rendimiento"""
        with threading.Lock():
            self.stats['jobs_processed'] += 1
            self.stats['total_files_processed'] += files_processed
            self.stats['total_processing_time'] += processing_time
            
            # Calcular promedio
            if self.stats['jobs_processed'] > 0:
                self.stats['average_job_time'] = (
                    self.stats['total_processing_time'] / self.stats['jobs_processed']
                )
            
            # Calcular throughput pico
            current_throughput = files_processed / processing_time if processing_time > 0 else 0
            if current_throughput > self.stats['peak_throughput']:
                self.stats['peak_throughput'] = current_throughput
    
    def submit_job(self, 
                   files: List[str], 
                   mode: ProcessingMode = ProcessingMode.FAST,
                   priority: int = 5) -> str:
        """Env√≠a un trabajo para procesamiento"""
        
        job_id = hashlib.md5(f"{time.time()}_{len(files)}".encode()).hexdigest()[:8]
        
        # Calcular tiempo estimado
        estimated_time = self._estimate_processing_time(files, mode)
        
        # Crear trabajo
        job = ProcessingJob(
            job_id=job_id,
            files=files,
            mode=mode,
            priority=priority,
            created_at=time.time(),
            estimated_time=estimated_time
        )
        
        # Agregar a la cola prioritaria
        self.job_queue.put((priority, job))
        
        logger.info(f"üìã Trabajo {job_id} enviado: {len(files)} archivos, modo {mode.value}")
        
        return job_id
    
    def _estimate_processing_time(self, files: List[str], mode: ProcessingMode) -> float:
        """Estima el tiempo de procesamiento"""
        base_time_per_file = {
            ProcessingMode.ULTRA_FAST: 0.001,
            ProcessingMode.FAST: 0.01,
            ProcessingMode.BALANCED: 0.05,
            ProcessingMode.QUALITY: 0.1
        }
        
        return len(files) * base_time_per_file[mode]
    
    def get_job_status(self, job_id: str) -> Dict:
        """Obtiene el estado de un trabajo"""
        # En una implementaci√≥n real, esto consultar√≠a una base de datos
        return {
            'job_id': job_id,
            'status': 'processing',
            'progress': 0.5,
            'estimated_completion': time.time() + 60
        }
    
    def get_performance_stats(self) -> Dict:
        """Obtiene estad√≠sticas de rendimiento"""
        return self.stats.copy()
    
    def shutdown(self):
        """Apaga el procesador paralelo"""
        logger.info("üõë Apagando procesador paralelo...")
        
        # Enviar se√±al de parada a workers
        for _ in range(len(self.workers)):
            self.job_queue.put(None)
        
        # Esperar a que terminen
        for worker in self.workers:
            if hasattr(worker, 'join'):
                worker.join(timeout=5)
        
        # Cerrar pools
        self.process_pool.shutdown(wait=True)
        self.thread_pool.shutdown(wait=True)
        
        logger.info("‚úÖ Procesador paralelo apagado")

class AsyncFileProcessor:
    """Procesador de archivos as√≠ncrono para m√°xima velocidad"""
    
    def __init__(self, max_concurrent: int = 100):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.stats = {
            'files_processed': 0,
            'total_time': 0,
            'errors': 0
        }
    
    async def process_files_async(self, file_paths: List[str]) -> List[Dict]:
        """Procesa archivos de forma as√≠ncrona"""
        start_time = time.time()
        
        # Crear tareas para todos los archivos
        tasks = [
            self._process_single_file_async(file_path)
            for file_path in file_paths
        ]
        
        # Ejecutar todas las tareas en paralelo
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Procesar resultados
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    'file_path': file_paths[i],
                    'success': False,
                    'error': str(result)
                })
                self.stats['errors'] += 1
            else:
                processed_results.append(result)
                self.stats['files_processed'] += 1
        
        # Actualizar estad√≠sticas
        self.stats['total_time'] = time.time() - start_time
        
        return processed_results
    
    async def _process_single_file_async(self, file_path: str) -> Dict:
        """Procesa un archivo individual de forma as√≠ncrona"""
        async with self.semaphore:
            try:
                # Simular procesamiento as√≠ncrono
                await asyncio.sleep(0.01)  # 10ms
                
                # Obtener informaci√≥n del archivo
                file_size = os.path.getsize(file_path)
                
                return {
                    'file_path': file_path,
                    'success': True,
                    'file_size': file_size,
                    'processed_at': time.time()
                }
                
            except Exception as e:
                return {
                    'file_path': file_path,
                    'success': False,
                    'error': str(e)
                }

class DistributedProcessor:
    """Procesador distribuido para m√∫ltiples m√°quinas"""
    
    def __init__(self, nodes: List[str]):
        self.nodes = nodes
        self.active_nodes = set()
        self.node_stats = {}
    
    async def distribute_work(self, files: List[str]) -> Dict:
        """Distribuye trabajo entre nodos"""
        # Implementaci√≥n b√°sica de distribuci√≥n
        chunks = self._split_work(files, len(self.nodes))
        
        results = {}
        for i, (node, chunk) in enumerate(zip(self.nodes, chunks)):
            try:
                # Enviar trabajo al nodo
                node_result = await self._send_to_node(node, chunk)
                results[node] = node_result
            except Exception as e:
                logger.error(f"‚ùå Error enviando trabajo a nodo {node}: {e}")
                results[node] = {'error': str(e)}
        
        return results
    
    def _split_work(self, files: List[str], num_nodes: int) -> List[Tuple[str, List[str]]]:
        """Divide el trabajo entre nodos"""
        chunk_size = len(files) // num_nodes
        chunks = []
        
        for i in range(num_nodes):
            start = i * chunk_size
            end = start + chunk_size if i < num_nodes - 1 else len(files)
            chunks.append((self.nodes[i], files[start:end]))
        
        return chunks
    
    async def _send_to_node(self, node: str, files: List[str]) -> Dict:
        """Env√≠a trabajo a un nodo espec√≠fico"""
        # Simular env√≠o a nodo
        await asyncio.sleep(0.1)
        
        return {
            'node': node,
            'files_received': len(files),
            'status': 'processing'
        }

# Funci√≥n para crear procesador paralelo
def create_parallel_processor(max_processes: int = None,
                             max_threads: int = None,
                             enable_gpu: bool = False) -> ParallelProcessor:
    """Crea una instancia del procesador paralelo"""
    try:
        processor = ParallelProcessor(
            max_processes=max_processes,
            max_threads=max_threads,
            enable_gpu=enable_gpu
        )
        
        logger.info("‚úÖ Procesador paralelo creado exitosamente")
        return processor
        
    except Exception as e:
        logger.error(f"‚ùå Error creando procesador paralelo: {e}")
        return None

# Funci√≥n para procesamiento masivo
async def process_massive_files(file_paths: List[str], 
                               mode: ProcessingMode = ProcessingMode.ULTRA_FAST) -> Dict:
    """Procesa una cantidad masiva de archivos con m√°xima velocidad"""
    
    try:
        # Crear procesador paralelo
        processor = create_parallel_processor()
        if not processor:
            return {'success': False, 'error': 'No se pudo crear el procesador'}
        
        # Crear procesador as√≠ncrono
        async_processor = AsyncFileProcessor(max_concurrent=200)
        
        start_time = time.time()
        
        # Procesar archivos
        if mode == ProcessingMode.ULTRA_FAST:
            # Modo ultra r√°pido: procesamiento paralelo masivo
            job_id = processor.submit_job(file_paths, mode)
            
            # Esperar un poco y obtener resultados
            await asyncio.sleep(0.1)
            
            results = {
                'mode': 'ultra_fast',
                'job_id': job_id,
                'files_count': len(file_paths),
                'estimated_time': processor._estimate_processing_time(file_paths, mode)
            }
        else:
            # Otros modos: procesamiento as√≠ncrono
            results = await async_processor.process_files_async(file_paths)
        
        processing_time = time.time() - start_time
        
        # Obtener estad√≠sticas
        stats = processor.get_performance_stats()
        
        return {
            'success': True,
            'results': results,
            'processing_time': processing_time,
            'stats': stats,
            'throughput': len(file_paths) / processing_time if processing_time > 0 else 0
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en procesamiento masivo: {e}")
        return {'success': False, 'error': str(e)}

if __name__ == "__main__":
    # Probar procesador paralelo
    logger.info("üöÄ Probando procesador paralelo masivo")
    
    # Crear archivos de prueba
    test_files = [f"test_file_{i}.txt" for i in range(100)]
    for file_path in test_files:
        with open(file_path, 'w') as f:
            f.write(f"Test content {file_path}")
    
    try:
        # Procesar archivos masivamente
        result = asyncio.run(process_massive_files(test_files, ProcessingMode.ULTRA_FAST))
        
        if result['success']:
            logger.info("‚úÖ Procesamiento masivo exitoso")
            logger.info(f"üìä Throughput: {result['throughput']:.2f} archivos/segundo")
            logger.info(f"‚è±Ô∏è Tiempo total: {result['processing_time']:.2f}s")
        else:
            logger.error(f"‚ùå Error en procesamiento: {result['error']}")
    
    finally:
        # Limpiar archivos de prueba
        for file_path in test_files:
            try:
                os.remove(file_path)
            except:
                pass 